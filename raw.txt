import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import xgboost
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (accuracy_score, classification_report, roc_curve, auc)
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import (train_test_split, StratifiedShuffleSplit, cross_val_score)
from sklearn.feature_selection import RFE

# loading dataset
attrition = pd.read_csv('employee.csv')

# the first few rows of the dataset
attrition.head()

np.random.seed(0)

# Data visualization using Seaborn and Plotly
f, axes = plt.subplots(3, 3, figsize=(10, 8), sharex=False, sharey=False)

# KDE Plots
s = np.linspace(0, 3, 10)
cmap = sns.cubehelix_palette(start=0.0, light=1, as_cmap=True)
sns.kdeplot(data=attrition, x='Age', y='TotalWorkingYears', cmap=cmap, fill=True, cut=5, ax=axes[0,0])
axes[0,0].set(title='Age against Total working years')

cmap = sns.cubehelix_palette(start=0.333333333333, light=1, as_cmap=True)
sns.kdeplot(data=attrition, x='Age', y='DailyRate', cmap=cmap, fill=True, ax=axes[0,1])
axes[0,1].set(title='Age against Daily Rate')

cmap = sns.cubehelix_palette(start=0.666666666667, light=1, as_cmap=True)
sns.kdeplot(data=attrition, x='YearsInCurrentRole', y='Age', cmap=cmap, fill=True, ax=axes[0,2])
axes[0,2].set(title='Years in role against Age')

cmap = sns.cubehelix_palette(start=1.0, light=1, as_cmap=True)
sns.kdeplot(data=attrition, x='DailyRate', y='DistanceFromHome', cmap=cmap, fill=True, ax=axes[1,0])
axes[1,0].set(title='Daily Rate against Distance from Home')

cmap = sns.cubehelix_palette(start=1.333333333333, light=1, as_cmap=True)
sns.kdeplot(data=attrition, x='DailyRate', y='JobSatisfaction', cmap=cmap, fill=True, ax=axes[1,1])
axes[1,1].set(title='Daily Rate against Job Satisfaction')

cmap = sns.cubehelix_palette(start=1.666666666667, light=1, as_cmap=True)
sns.kdeplot(data=attrition, x='YearsAtCompany', y='JobSatisfaction', cmap=cmap, fill=True, ax=axes[1,2])
axes[1,2].set(title='Years at Company against Job Satisfaction')

cmap = sns.cubehelix_palette(start=2.0, light=1, as_cmap=True)
sns.kdeplot(data=attrition, x='YearsAtCompany', y='DailyRate', cmap=cmap, fill=True, ax=axes[2,0])
axes[2,0].set(title='Years at Company against Daily Rate')

cmap = sns.cubehelix_palette(start=2.333333333333, light=1, as_cmap=True)
sns.kdeplot(data=attrition, x='RelationshipSatisfaction', y='YearsWithCurrManager', cmap=cmap, fill=True, ax=axes[2,1])
axes[2,1].set(title='Relationship Satisfaction vs Years with Manager')

cmap = sns.cubehelix_palette(start=2.666666666667, light=1, as_cmap=True)
sns.kdeplot(data=attrition, x='WorkLifeBalance', y='JobSatisfaction', cmap=cmap, fill=True, ax=axes[2,2])
axes[2,2].set(title='Work Life Balance against Job Satisfaction')

f.tight_layout()
plt.show()


# Define a dictionary for the target mapping
target_map = {'Yes':1, 'No':0}
# Use the pandas apply method to numerically encode our attrition target variable
attrition["Attrition_numerical"] = attrition["Attrition"].apply(lambda x: target_map[x])

# creating a list of only numerical values
numerical = [u'Age', u'DailyRate', u'DistanceFromHome', 
             u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction',
             u'HourlyRate', u'JobInvolvement', u'JobLevel', u'JobSatisfaction',
             u'MonthlyIncome', u'MonthlyRate', u'NumCompaniesWorked',
             u'PercentSalaryHike', u'PerformanceRating', u'RelationshipSatisfaction',
             u'StockOptionLevel', u'TotalWorkingYears',
             u'TrainingTimesLastYear', u'WorkLifeBalance', u'YearsAtCompany',
             u'YearsInCurrentRole', u'YearsSinceLastPromotion',u'YearsWithCurrManager']
data = [
    go.Heatmap(
        z= attrition[numerical].astype(float).corr().values, # Generating the Pearson correlation
        x=attrition[numerical].columns.values,
        y=attrition[numerical].columns.values,
        colorscale='Viridis',
        reversescale = False,
#         text = True ,
        opacity = 1.0
        
    )
]


layout = go.Layout(
    title='Pearson Correlation of numerical features',
    xaxis = dict(ticks='', nticks=36),
    yaxis = dict(ticks='' ),
    width = 900, height = 700,
    
)


fig = go.Figure(data=data, layout=layout)
py.iplot(fig, filename='labelled-heatmap')


# Define target mapping and encode target variable
target_map = {'Yes':1, 'No':0}
attrition["Attrition_numerical"] = attrition["Attrition"].apply(lambda x: target_map[x])

# Drop the 'Attrition_numerical' column
attrition = attrition.drop(['Attrition_numerical'], axis=1)

# Categorical data analysis
categorical = [col for col in attrition.columns if attrition[col].dtype == 'object']
numerical = attrition.columns.difference(categorical)

# One-Hot Encoding of categorical variables
attrition_cat = attrition[categorical].drop(['Attrition'], axis=1)
attrition_cat = pd.get_dummies(attrition_cat)
attrition_num = attrition[numerical]
attrition_final = pd.concat([attrition_num, attrition_cat], axis=1)

target = attrition["Attrition"].apply(lambda x: target_map[x])

# Split data into train and test sets
train, test, target_train, target_val = train_test_split(attrition_final, target, train_size=0.80, random_state=0)

# Alternatively, using StratifiedShuffleSplit
strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=0)
for train_index, test_index in strat_split.split(attrition_final, target):
    train, test = attrition_final.iloc[train_index], attrition_final.iloc[test_index]
    target_train, target_val = target.iloc[train_index], target.iloc[test_index]

# Apply SMOTE to balance the dataset
oversampler = SMOTE(random_state=0)
smote_train, smote_target = oversampler.fit_resample(train, target_train)

# Initialize and train the Random Forest model
rf_params = {
    'n_jobs': -1,
    'n_estimators': 1000,
    'max_features': 'sqrt',
    'max_depth': 4,
    'min_samples_leaf': 2,
    'random_state': 0,
    'verbose': 0
}
rf = RandomForestClassifier(**rf_params)
rf.fit(smote_train, smote_target)

# Making predictions on the test set
rf_predictions = rf.predict(test)
print("Random Forest Accuracy score: {}".format(accuracy_score(target_val, rf_predictions)))
print("="*80)
print(classification_report(target_val, rf_predictions))

# Feature importance plot for Random Forest
trace = go.Scatter(
    y=rf.feature_importances_,
    x=attrition_final.columns.values,
    mode='markers',
    marker=dict(
        sizemode='diameter',
        sizeref=1,
        size=13,
        color=rf.feature_importances_,
        colorscale='Portland',
        showscale=True
    ),
    text=attrition_final.columns.values
)
data = [trace]

layout = go.Layout(
    autosize=True,
    title='Random Forest Feature Importance',
    hovermode='closest',
    xaxis=dict(
        ticklen=5,
        showgrid=False,
        zeroline=False,
        showline=False
    ),
    yaxis=dict(
        title='Feature Importance',
        showgrid=False,
        zeroline=False,
        ticklen=5,
        gridwidth=2
    ),
    showlegend=False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig, filename='scatter2010')

# Initialize and train the Gradient Boosting model
gb_params = {
    'n_estimators': 1000,
    'learning_rate': 0.01,
    'max_depth': 4,
    'min_samples_leaf': 2,
    'random_state': 0
}
gb = GradientBoostingClassifier(**gb_params)
gb.fit(smote_train, smote_target)

# Making predictions on the test set
gb_predictions = gb.predict(test)
print("Gradient Boosting Accuracy score: {}".format(accuracy_score(target_val, gb_predictions)))
print("="*80)
print(classification_report(target_val, gb_predictions))

# Initialize and train the XGBoost model
xgb_params = {
    'objective': 'binary:logistic',
    'max_depth': 4,
    'n_estimators': 1000,
    'learning_rate': 0.01,
    'random_state': 0
}
xgb_model = xgboost.XGBClassifier(**xgb_params)
xgb_model.fit(smote_train, smote_target)

# Making predictions on the test set
xgb_predictions = xgb_model.predict(test)
print("XGBoost Accuracy score: {}".format(accuracy_score(target_val, xgb_predictions)))
print("="*80)
print(classification_report(target_val, xgb_predictions))

# ROC Curves
def plot_roc_curve(y_true, y_scores, label=None):
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc:.2f})')

plt.figure(figsize=(10, 8))
plot_roc_curve(target_val, rf.predict_proba(test)[:, 1], 'Random Forest')
plot_roc_curve(target_val, gb.predict_proba(test)[:, 1], 'Gradient Boosting')
plot_roc_curve(target_val, xgb_model.predict_proba(test)[:, 1], 'XGBoost')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc='lower right')
plt.show()

# Cross-Validation Scores
models = [('Random Forest', rf), ('Gradient Boosting', gb), ('XGBoost', xgb_model)]
for name, model in models:
    scores = cross_val_score(model, attrition_final, target, cv=5, scoring='accuracy')
    print(f'{name} Cross-Validation Accuracy: {scores.mean():.2f} Â± {scores.std():.2f}')

# Feature Selection with RFE
rfe = RFE(rf, n_features_to_select=10)
rfe = rfe.fit(smote_train, smote_target)
print(f'Selected features: {attrition_final.columns[rfe.support_]}')

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 500, 1000],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.1, 0.2, 0.3]
}

# Initialize the model
xgb_model = xgboost.XGBClassifier(random_state=0)

# Initialize GridSearchCV
grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,
    n_jobs=-1,
    verbose=1
)

# Fit GridSearchCV
grid_search.fit(smote_train, smote_target)

# Best parameters and best score
print("Best parameters found: ", grid_search.best_params_)
print("Best score found: ", grid_search.best_score_)

from sklearn.model_selection import RandomizedSearchCV

# Define the parameter distributions
param_dist = {
    'n_estimators': [100, 200, 500, 1000],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.1, 0.2, 0.3]
}

# Initialize the model
xgb_model = xgboost.XGBClassifier(random_state=0)

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=20,
    scoring='accuracy',
    cv=5,
    n_jobs=-1,
    verbose=1,
    random_state=0
)

# Fit RandomizedSearchCV
random_search.fit(smote_train, smote_target)

# Best parameters and best score
print("Best parameters found: ", random_search.best_params_)
print("Best score found: ", random_search.best_score_)

from hyperopt import fmin, tpe, hp, Trials

def objective(params):
    model = xgboost.XGBClassifier(
        n_estimators=int(params['n_estimators']),
        max_depth=int(params['max_depth']),
        learning_rate=params['learning_rate'],
        random_state=0
    )
    model.fit(smote_train, smote_target)
    score = model.score(test, target_val)
    return -score

space = {
    'n_estimators': hp.choice('n_estimators', [100, 200, 500, 1000]),
    'max_depth': hp.choice('max_depth', [3, 5, 7, 10]),
    'learning_rate': hp.uniform('learning_rate', 0.01, 0.3)
}

trials = Trials()
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)

print("Best parameters found: ", best)

from xgboost import XGBClassifier

# Best parameters from Grid Search
best_params = {
    'learning_rate': 0.01,
    'max_depth': 5,
    'n_estimators': 500
}

# Initialize and train the final model with best parameters
final_model = XGBClassifier(
    learning_rate=best_params['learning_rate'],
    max_depth=best_params['max_depth'],
    n_estimators=best_params['n_estimators'],
    random_state=0
)
final_model.fit(smote_train, smote_target)

# Evaluate the final model
final_predictions = final_model.predict(test)
print("Final Model Accuracy score: {}".format(accuracy_score(target_val, final_predictions)))
print("="*80)
print(classification_report(target_val, final_predictions))